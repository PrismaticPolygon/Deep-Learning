# -*- coding: utf-8 -*-
"""ACAI_II.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jqlEQqf_vm5DW0CN3vL1NuoHIcvPOwjz

IPython magic. This script saves output images to my Google Drive - this needs disabling for submission.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# from os.path import exists
# from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
# from google.colab import drive
# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
# cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
# accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'
# !pip install -q torch torchvision livelossplot
# drive.mount("/content/gdrive")
# from livelossplot import PlotLosses

"""Imports. Nothing special."""

import math
import time
import torch
import random
import os
import torchvision.transforms as transforms
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

from torchvision.datasets import CIFAR10
from torch.utils.data.sampler import Sampler
from torch.nn.utils import spectral_norm
from torchvision.utils import make_grid
from torch.utils.data import DataLoader
import datetime
from torch.optim import Adam, SGD
from torchvision.utils import make_grid
from functools import partial

"""Arguments for operation. Only "disc_train" is not an original parameter."""

args = {
    "epochs": 150,          # Number of epochs
    "batch_size": 64,       # Batch size
    "depth": 32,            # Depth of first forward convolution
    "latent_depth": 32,     # Latent space depth. Total latent size is latent * latent_width ** 2.
    "lr": 1e-3,             # Learning rate
    "advdepth": 16,         # The depth of discriminator network
    "advweight": 0.3,       # Otherwise 0.5
    "reg": 0.2,             # Amount of discriminator regularisation
    "weight_decay": 0,      # The rate at which learned weights decay
    "width": 32,            # ?
    "latent_width": 4,      # Latent space is of shape (latent_depth, latent_width, latent_width)
    "device": "cuda",       # Device to use.
    "disc_train": 0,        # How many rounds to overtrain the discriminator for spectral normalisation
}

# I can definitely work on the image.
# It's still shite!

args["scales"] = int(math.log2(args["width"] / args["latent_width"]))   # How many convolutional blocks to use in AE
args["advdepth"] = args["advdepth"] or args["depth"]                    # Don't allow advdepth of 0

"""Custom dataset. Combine deer / horses with birds / planes. The first 10000 images are 'bodies'. The second 10000 are 'wings'. Similarly, the sampler yields batches with the first half bodies and the second half wings."""

class Pegasus(CIFAR10):

    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):

        super().__init__(root, train, transform, target_transform, download)

        # wing_indices = [0, 2] # Planes and birds 
        # body_indices = [4, 7] # Deer and horses

        wing_indices = [0]
        body_indices = [7]

        indices = np.arange(len(self.targets))

        body_indices = np.array([i for i in indices if self.targets[i] in body_indices])
        wing_indices = np.array([i for i in indices if self.targets[i] in wing_indices])

        body_data = np.take(self.data, body_indices, axis=0)
        wing_data = np.take(self.data, wing_indices, axis=0)

        body_targets = np.take(self.targets, body_indices, axis=0)
        wing_targets = np.take(self.targets, wing_indices, axis=0)

        # np.random.shuffle(body_data)
        # np.random.shuffle(wing_data)

        self.data = np.vstack((body_data, wing_data))
        self.targets = np.hstack((body_targets, wing_targets))


# https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html
class PegasusSampler(Sampler):

    def __init__(self, data_source, batch_size):

        super().__init__(data_source)

        self.data_source = data_source
        self.batch_size = batch_size

    def __iter__(self):

        batch_size = self.batch_size // 2

        for n in range(len(self)):  # 156 batches

            batch = [i for i in range(n * batch_size, (n + 1) * batch_size)]
            batch += [len(self.data_source) - i - 1 for i in range(n * batch_size, (n + 1) * batch_size)]

            yield batch

    def __len__(self):

        return len(self.data_source) // self.batch_size

"""Encoder. Pass a ```use_spectral_norm``` flag for use in the discriminator."""

def Encoder(scales, depth, latent, use_spectral_norm=False):

    activation = partial(nn.LeakyReLU, negative_slope=0.2)
    kernel_size = 3
    in_channels = depth

    layers = [
        nn.Conv2d(3, depth, 1, padding=1)
    ]

    for scale in range(scales):

        out_channels = depth << scale

        if use_spectral_norm:

            layers.extend([
                spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)),
                activation(),
                spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size, padding=1)),
                activation(),
                nn.AvgPool2d(2)
            ])

        else:

            layers.extend([
                nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),
                activation(),
                nn.Conv2d(out_channels, out_channels, kernel_size, padding=1),
                activation(),
                nn.AvgPool2d(2)
            ])

        in_channels = out_channels

    out_channels = depth << scales

    if use_spectral_norm:

        layers.extend([
            spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)),
            activation(),
            spectral_norm(nn.Conv2d(out_channels, latent, kernel_size, padding=1))
        ])

    else:

        layers.extend([
            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),
            activation(),
            nn.Conv2d(out_channels, latent, kernel_size, padding=1)
        ])

    return nn.Sequential(*layers)

"""Decoder"""

def Decoder(scales, depth, latent):

    activation = partial(nn.LeakyReLU, negative_slope=0.2)
    kernel_size = 3
    in_channels = latent

    layers = []

    for scale in range(scales - 1, -1, -1):     # Descend from 64 to 16

        out_channels = depth << scale

        layers.extend([
            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),
            activation(),
            nn.Conv2d(out_channels, out_channels, kernel_size, padding=1),
            activation(),
            nn.Upsample(scale_factor=2)
        ])

        in_channels = out_channels

    layers.extend([
        nn.Conv2d(in_channels, depth, kernel_size, padding=1),
        activation(),
        nn.Conv2d(depth, 3, kernel_size, padding=1),
        nn.Sigmoid()    # To convert output to [0, 1]
    ])

    return nn.Sequential(*layers)

"""Autoencoder and discrimimator"""

class AutoEncoder(nn.Module):

    def __init__(self, scales, depth, latent):

        super().__init__()

        self.encoder = Encoder(scales, depth, latent)
        self.decoder = Decoder(scales, depth, latent)

    def forward(self, x):

        encoded = self.encoder(x)
        decoded = self.decoder(encoded)

        return encoded, decoded

class Discriminator(nn.Module):

    def __init__(self, scales, advdepth, latent):

        super().__init__()

        self.encoder = Encoder(scales, advdepth, latent, use_spectral_norm=True)

    def forward(self, x):

        x = self.encoder(x)  # (batch, 16, 4, 4)

        return torch.mean(x, [1, 2, 3])  # (batch)

"""Load the dataset and create the iterator."""

def cycle(iterable):

    while True:

        for x in iterable:

            yield x


transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
])

train_set = Pegasus(root='./data', train=True, download=True, transform=transform_train)
train_loader = DataLoader(train_set, batch_sampler=PegasusSampler(train_set, batch_size=args["batch_size"]))

test_set = Pegasus(root='./data', train=False, download=True, transform=transform_train)
test_loader = DataLoader(test_set, batch_sampler=PegasusSampler(test_set, batch_size=args["batch_size"]))

train_iterator = iter(cycle(train_loader))
test_iterator = iter(cycle(test_loader))

"""Helper functions to calculate the loss of the discriminator and autoencoder, and to save images to Google Drive. Parameter names follow as closely as possible the original paper."""

criterion_disc = nn.MSELoss()


def calc_loss_disc(x, x_hat, discriminator, disc_mix, alpha):
   
    gamma = args["reg"]

    loss = criterion_disc(disc_mix.squeeze(), alpha.squeeze())                                    # || d_omega(x^_alpha) - alpha||^2
    regulariser = torch.mean(discriminator(gamma * x + (1 - gamma) * x_hat)) ** 2       # || d_omega(gamma * x + (1 - gamma) x^) ||^2

    return loss + regulariser


criterion_ae = nn.BCELoss()


def calc_loss_ae(x, x_hat, disc_mix):
    
    loss = criterion_ae(x_hat, x)                                               # ||x - g_phi(f_theta(x))||^2
    regulariser = args["advweight"] * (torch.mean(disc_mix) ** 2)               # lambda * || d_omega(x^_alpha) ||^2

    return loss + regulariser

def imshow(tensor):

    img = make_grid(tensor)

    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')

# This IS cool.
# I think we want twofold.
# One should be the actual image
# I could probably just concatenate the numpy images, actually.
# No. We need some alpha stuff.
# I really don't back the mean.

def imsave(tensor, y, alpha, preds, filename=None):

    y = y.detach().cpu().numpy()
    alpha = alpha.detach().cpu().squeeze().numpy()
    preds = preds.detach().cpu().squeeze().numpy()
    tensor = tensor.detach().cpu().numpy()

    transposed = np.transpose(tensor, (0, 2, 3, 1)) # Move colour to last channel

    plt.figure(figsize=(15, 20))

    for i, img in enumerate(transposed): # A (3, 32, 32) image

        plt.subplot(7, 5, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)

        label = ""

        if y[i] == 4:

            label += "deer/"

        elif y[i] == 7:

            label += "horse/"

        if y[args["batch_size"] - 1 - i] == 0:

            label += "planes"

        elif y[args["batch_size"] - 1 - i] == 2:

            label += "bird"

        label += " ({:.4f}, {:.4f})".format(alpha[i], preds[i])

        plt.xlabel(label)
        plt.imshow(img, cmap=plt.cm.binary)

    plt.show()

    if filename is not None:

      plt.savefig("/content/gdrive/My Drive/ACAI/" + filename + ".png")

"""Initialisation of the autoencoder, discriminator and their respective optimisers. Print out their structures just to be safe."""

ae = AutoEncoder(args["scales"], args["depth"], args["latent_depth"]).to(args["device"])
d = Discriminator(args["scales"], args['advdepth'], args['latent_depth']).to(args['device'])

print("#### AUTOENCODER ###\n")
print(ae, end="\n\n")

print("### DISCRIMINATOR ###\n")
print(d, end="\n\n")

# Optimiser for autoencoder parameters
optimiser_ae = Adam(
    ae.parameters(),
    lr=args["lr"],
    weight_decay=args["weight_decay"]
)

# Optimiser for discriminator parameters
optimiser_d = Adam(
    d.parameters(),
    lr=args["lr"],
    weight_decay=args["weight_decay"]
)

"""The main loop. Prints out the current generated pegasi batch and the autoencoder and discriminator losses."""

# liveloss = PlotLosses()

for epoch in range(args["epochs"]):

    logs = {}

    loss_ae_sum = 0
    loss_d_sum = 0
    start = time.time()

    num = len(train_loader) // (1 + args["disc_train"])     # 52 = 312 / (1 + 5)
    k = random.randint(0, num)                              # Random batch for variety

    for i in range(num):

        # Overtrain discriminator
        for j in range(args["disc_train"]):

            x, _ = next(train_iterator)
            x = x.to(args["device"])    # Input images
            z, x_hat = ae(x)            # Encoded and decoded images
            half = args["batch_size"] // 2

            # Train on bodies. Bodies are combinations with an alpha of 0
            alpha = torch.zeros(half, 1, 1, 1).to(args["device"])
            loss_bodies = calc_loss_disc(x[half:], x_hat[half:], d, alpha, alpha)

            # Train on wings. Wings are combinations with an alpha of 1
            alpha = torch.ones(half, 1, 1, 1).to(args["device"])
            loss_wings = calc_loss_disc(x[:half], x_hat[:half], d, alpha, alpha)

            loss_d = (loss_bodies + loss_wings) / 2.0

            optimiser_d.zero_grad()
            loss_d.backward(retain_graph=True)
            optimiser_d.step()

        # Train entire network
        x, y = next(train_iterator)

        x = x.to(args["device"])

        z, x_hat = ae(x)
        half = args["batch_size"] // 2

        alpha = torch.rand(half, 1, 1, 1).to(args['device']) / 2

        bodies = z[half:]
        wings = z[:half]

        x_hat_alpha = ae.decoder(alpha * wings + (1 - alpha) * bodies)                 # Decoded combined latent space
        disc_mix = d(x_hat_alpha)                # Estimates of alpha

        loss_ae = calc_loss_ae(x, x_hat, disc_mix)

        optimiser_ae.zero_grad()
        loss_ae.backward(retain_graph=True)
        optimiser_ae.step()

        loss_ae_sum += loss_ae.item()

        loss_d = calc_loss_disc(x, x_hat, d, disc_mix, alpha)

        optimiser_d.zero_grad()
        loss_d.backward(retain_graph=True)
        optimiser_d.step()

        loss_d_sum += loss_d.item()

        if i == k:    # Save random batch

            imsave(x_hat_alpha, y, alpha, disc_mix, "images/{}-{}.png".format(datetime.datetime.today(), epoch))

    logs = {
        "ae": loss_ae_sum / num,
        "disc": loss_d_sum / num
    }

    print("{}: {}".format(epoch, logs))

    torch.save(ae.state_dict(), "weights/autoencoder_horse_plane_1.pkl")
    torch.save(d.state_dict(), "weights/discriminator_horse_plane_1.pkl")

"""For testing purposes. Save the state of both networks, and produce pegasi from the training set. For each, save the image individually."""

print("Training complete!")

ae = AutoEncoder(args["scales"], args["depth"], args["latent_depth"]).to(args["device"])
d = Discriminator(args["scales"], args['advdepth'], args['latent_depth']).to(args['device'])

ae.load_state_dict(torch.load("/content/gdrive/My Drive/ACAI/weights/autoencoder_bird.pkl"))
ae.eval()

d.load_state_dict(torch.load("/content/gdrive/My Drive/ACAI/weights/discriminator_bird.pkl"))
d.eval()

# THese are pretty garbage.
# No matter; we're still going to submit.

for i in range(len(test_loader)):

    x, y = next(test_iterator)

    x = x.to(args["device"])
    z, x_hat = ae(x)
    half = args["batch_size"] // 2

    alpha = torch.rand(half, 1, 1, 1).to(args['device']) / 2

    horses = z[half:]
    birds = z[:half]

    x_hat_alpha = ae.decoder(alpha * birds + (1 - alpha) * horses)  # Decoded combined latent space

    disc_mix = d(x_hat_alpha)  # Estimates of alpha

    imsave(x_hat_alpha, y, alpha, disc_mix, filename="pegasi/{}".format(i))