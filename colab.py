# -*- coding: utf-8 -*-
"""ACAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12sg1T3hh0APuvUPE7Ym8-rFOG0XgCkat
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# from os.path import exists
# from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
# from google.colab import drive
# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
# cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
# accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'
# !pip install -q torch torchvision livelossplot
# drive.mount("/content/gdrive")

import math
import time
import torch
import torchvision.transforms as transforms
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt


from torchvision.datasets import CIFAR10
from torch.utils.data.sampler import Sampler
from torch.nn.utils import spectral_norm
from torch.utils.data import DataLoader
from torch.optim import Adam
from torchvision.utils import make_grid

# from livelossplot import PlotLosses


args = {
    "epochs": 500,          # Number of epochs
    "batch_size": 64,       # Batch size
    "depth": 32,            # Depth of first forward convolution
    "latent_depth": 16,     # Latent space depth. Total latent size is latent * latent_width ** 2.
    "lr": 1e-3,             # Learning rate
    "advdepth": 0,          # The depth of discriminator network
    "advweight": 0.5,       # Otherwise 0.5
    "reg": 0.2,             # Amount of discriminator regularisation
    "weight_decay": 0,      # The rate at which learned weights decay
    "width": 32,            # ?
    "latent_width": 4,      # Latent space is of shape (latent_depth, latent_width, latent_width)
    "device": "cuda",       # Device to use.
    "wings": "planes",       # Which image class to get wings from
    "disc_train": 0,        # How many rounds to overtrain the discriminator for spectral normalisation
}

args["scales"] = int(math.log2(args["width"] / args["latent_width"]))   # How many convolutional blocks to use in AE
args["advdepth"] = args["advdepth"] or args["depth"]                    # Don't allow advdepth of 0

# https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10
class Pegasus(CIFAR10):

    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):

        super().__init__(root, train, transform, target_transform, download)

        if args["wings"] == "birds":

            bird_index = 2  # Birds

        else:

            bird_index = 0  # Planes

        horse_index = 7

        indices = np.arange(len(self.targets))

        horse_indices = np.array([i for i in indices if self.targets[i] == horse_index])
        bird_indices = np.array([i for i in indices if self.targets[i] == bird_index])

        bird_data = np.take(self.data, bird_indices, axis=0)
        horse_data = np.take(self.data, horse_indices, axis=0)

        np.random.shuffle(bird_data)
        np.random.shuffle(horse_data)

        self.data = np.vstack((bird_data, horse_data))
        self.targets = np.zeros((10000, 1), dtype=np.uint8)

        self.targets[:5000] = bird_index
        self.targets[5000:] = horse_index


# https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html
class PegasusSampler(Sampler):

    def __init__(self, data_source, batch_size):

        super().__init__(data_source)

        self.data_source = data_source
        self.batch_size = batch_size

    def __iter__(self):

        batch_size = self.batch_size // 2

        for n in range(len(self)):  # 156 batches

            batch = [i for i in range(n * batch_size, (n + 1) * batch_size)]
            batch += [len(self.data_source) - i - 1 for i in range(n * batch_size, (n + 1) * batch_size)]

            yield batch

    def __len__(self):

        return len(self.data_source) // self.batch_size


def initialiser(layers, slope=0.2):

    for layer in layers:

        if hasattr(layer, 'weight'):

            w = layer.weight.data
            std = 1 / np.sqrt((1 + slope ** 2) * np.prod(w.shape[:-1]))
            w.normal_(std=std)

        if hasattr(layer, 'bias'):

            layer.bias.data.zero_()


def Encoder(scales, depth, latent, use_spectral_norm=False):

    activation = nn.LeakyReLU
    kernel_size = 3
    in_channels = depth

    layers = [
        nn.Conv2d(3, depth, 1, padding=1)
    ]

    for scale in range(scales):

        out_channels = depth << scale

        if use_spectral_norm:

            layers.extend([
                spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)),
                activation(),
                spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size, padding=1)),
                activation(),
                nn.AvgPool2d(2)
            ])

        else:

            layers.extend([
                nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),
                activation(),
                nn.Conv2d(out_channels, out_channels, kernel_size, padding=1),
                activation(),
                nn.AvgPool2d(2)
            ])

        in_channels = out_channels

    out_channels = depth << scales

    if use_spectral_norm:

        layers.extend([
            spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)),
            activation(),
            spectral_norm(nn.Conv2d(out_channels, latent, kernel_size, padding=1))
        ])

    else:

        layers.extend([
            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),
            activation(),
            nn.Conv2d(out_channels, latent, kernel_size, padding=1)
        ])

    initialiser(layers)

    return nn.Sequential(*layers)


def Decoder(scales, depth, latent):

    activation = nn.LeakyReLU
    kernel_size = 3
    in_channels = latent

    layers = []

    for scale in range(scales - 1, -1, -1):     # Descend from 64 to 16

        out_channels = depth << scale

        layers.extend([
            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),
            activation(),
            nn.Conv2d(out_channels, out_channels, kernel_size, padding=1),
            activation(),
            nn.Upsample(scale_factor=2)
        ])

        in_channels = out_channels

    layers.extend([
        nn.Conv2d(in_channels, depth, kernel_size, padding=1),
        activation(),
        nn.Conv2d(depth, 3, kernel_size, padding=1),
        nn.Sigmoid()    # To convert output to [0, 1]
    ])

    initialiser(layers)

    return nn.Sequential(*layers)


class AutoEncoder(nn.Module):

    def __init__(self, scales, depth, latent):

        super().__init__()

        self.encoder = Encoder(scales, depth, latent)
        self.decoder = Decoder(scales, depth, latent)

    def forward(self, x):

        encoded = self.encoder(x)
        decoded = self.decoder(encoded)

        return encoded, decoded

class Discriminator(nn.Module):

    def __init__(self, scales, advdepth, latent):

        super().__init__()

        self.encoder = Encoder(scales, advdepth, latent, use_spectral_norm=True)

    def forward(self, x):

        x = self.encoder(x)                 # (batch, 3, 32, 32) -> (batch, 16, 4, 4)

        return torch.mean(x, [1, 2, 3])     # (batch, 16, 4, 4) -> (batch)


def cycle(iterable):

    while True:

        for x in iterable:

            yield x


transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
])

train_set = Pegasus(root='./data', train=True, download=True, transform=transform_train)
train_loader = DataLoader(train_set, batch_sampler=PegasusSampler(train_set, batch_size=args["batch_size"]))

train_iterator = iter(cycle(train_loader))

test_set = Pegasus(root='./data', train=False, download=True, transform=transform_train)
test_loader = DataLoader(test_set, batch_sampler=PegasusSampler(train_set, batch_size=args["batch_size"]))

test_iterator = iter(cycle(test_loader))


criterion_disc = nn.MSELoss()


def calc_loss_disc(x, x_hat, discriminator, disc_mix, alpha):
   
    gamma = args["reg"]

    loss = criterion_disc(disc_mix.squeeze(), alpha.squeeze())                                    # || d_omega(x^_alpha) - alpha||^2
    regulariser = torch.mean(discriminator(gamma * x + (1 - gamma) * x_hat)) ** 2       # || d_omega(gamma * x + (1 - gamma) x^) ||^2

    return loss + regulariser


criterion_ae = nn.BCELoss()


def calc_loss_ae(x, x_hat, disc_mix):
    
    loss = criterion_ae(x_hat, x)                                               # ||x - g_phi(f_theta(x))||^2
    regulariser = args["advweight"] * (torch.mean(disc_mix) ** 2)               # lambda * || d_omega(x^_alpha) ||^2

    return loss + regulariser


def imsave(tensor, filename):

    tensor = tensor.detach().cpu()
    img = make_grid(tensor)

    np_img = img.numpy()
    transposed = np.transpose(np_img, (1, 2, 0))

    plt.imshow(transposed, interpolation='nearest')  # Expects(M, N, 3)

    # plt.savefig("/content/gdrive/My Drive/ACAI/" + filename + ".png")
    plt.savefig("images/acai/" + filename + ".png")


ae = AutoEncoder(args["scales"], args["depth"], args["latent_depth"]).to(args["device"])
d = Discriminator(args["scales"], args['advdepth'], args['latent_depth']).to(args['device'])

# Optimiser for autoencoder parameters
optimiser_ae = Adam(
    ae.parameters(),
    lr=args["lr"],
    weight_decay=args["weight_decay"]
)

# Optimiser for discriminator parameters
optimiser_d = Adam(
    d.parameters(),
    lr=args["lr"],
    weight_decay=args["weight_decay"]
)

# liveloss = PlotLosses()

for epoch in range(args["epochs"]):

    logs = {}

    loss_ae_sum = 0
    loss_d_sum = 0
    start = time.time()
    num = len(train_loader) // (1 + args["disc_train"])     # 26 = 156 / (1 + 5)

    for i in range(num):

        # Overtrain discriminator
        for j in range(args["disc_train"]):

            x, _ = next(train_iterator)
            x = x.to(args["device"])    # Input images
            z, x_hat = ae(x)            # Encoded and decoded images
            half = args["batch_size"] // 2

            # Train on horses. Horses are combinations with an alpha of 0
            alpha = torch.zeros(half, 1, 1, 1).to(args["device"])
            loss_horses = calc_loss_disc(x[half:], x_hat[half:], d, alpha, alpha)

            # Train on birds. Birds are combinations with an alpha of 1
            alpha = torch.ones(half, 1, 1, 1).to(args["device"])
            loss_birds = calc_loss_disc(x[:half], x_hat[:half], d, alpha, alpha)

            loss_d = (loss_horses + loss_birds) / 2.0

            optimiser_d.zero_grad()
            loss_d.backward(retain_graph=True)
            optimiser_d.step()

        # Train entire network
        x, _ = next(train_iterator)
        x = x.to(args["device"])

        z, x_hat = ae(x)
        half = args["batch_size"] // 2

        alpha = torch.rand(half, 1, 1, 1).to(args['device']) / 2

        horses = z[half:]
        birds = z[:half]

        x_hat_alpha = ae.decoder(alpha * birds + (1 - alpha) * horses)                 # Decoded combined latent space

        disc_mix = d(x_hat_alpha)                # Estimates of alpha

        loss_ae = calc_loss_ae(x, x_hat, disc_mix)

        optimiser_ae.zero_grad()
        loss_ae.backward(retain_graph=True)
        optimiser_ae.step()

        loss_ae_sum += loss_ae.item()

        loss_d = calc_loss_disc(x, x_hat, d, disc_mix, alpha)

        optimiser_d.zero_grad()
        loss_d.backward(retain_graph=True)
        optimiser_d.step()

        loss_d_sum += loss_d.item()

        if i == num - 1:    # Last batch

            imsave(x, "x_{}".format(epoch + 1))
            imsave(x_hat, "x_hat_{}".format(epoch + 1))
            imsave(x_hat_alpha, "x_hat_alpha_{}".format(epoch + 1))

    loss_ae_sum /= num
    loss_d_sum /= num

    logs["ae"] = loss_ae_sum
    logs["disc"] = loss_d_sum

    # liveloss.update(logs)
    # liveloss.draw()
    # I could also conceivably do birds and horses, though I think that this might confuse it.
    #

    print("{}/{}: {:.4f}, {:.4f} ({:.2f}s)".format(epoch + 1, args["epochs"], loss_ae_sum, loss_d_sum, time.time() - start))

print("Training complete")

torch.save(ae.state_dict(), "./content/gdrive/My Drive/ACAI/autoencoder_bird.pkl")
torch.save(d.state_dict(), "./content/gdrive/My Drive/ACAI/discriminator_bird.pkl")

for x, y in train_loader:

    x = x.to(args["device"])
    z, x_hat = ae(x)
    half = args["batch_size"] // 2

    alpha = torch.rand(half, 1, 1, 1).to(args['device']) / 2

    horses = z[half:]
    birds = z[:half]

    x_hat_alpha = ae.decoder(alpha * birds + (1 - alpha) * horses)  # Decoded combined latent space

    disc_mix = d(x_hat_alpha)  # Estimates of alpha

    alpha = alpha.squeeze()

    for i, img in enumerate(x_hat_alpha):

        print(img.shape)

        imsave(img, "pegasi/{}-{}-{}".format(args["wings"], alpha[i], disc_mix[i]))

